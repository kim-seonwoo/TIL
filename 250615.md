### **인공지능 학습 및 핵심 개념 대비**

### **1. 학습하면서 숫자 계산하는 문제**

인공신경망 학습 과정에서는 **가중치(Weight)와 편향(Bias)을 활용한 수치 계산**이 필수적입니다. 특히 **단일 퍼셉트론의 연산**과 **다층 퍼셉트론의 순전파(Forward Pass) 및 오류 역전파(Backpropagation) 과정**에서 숫자 계산이 발생합니다. 또한 **불확실한 지식의 표현**에서는 **확신도(Certainty Factor) 계산**이 중요합니다.

- **단일 퍼셉트론 연산 (강의 4)**
  - **계산 방식**: 각 입력값(`x`)에 해당 **가중치(`w`)**를 곱하고, 여기에 **편향(`b`)**을 더하여 **합성값(`S`)**을 구합니다. `S = Σ(wi * xi) + b`. 이 합성값은 다시 **활성화 함수(Activation Function, `f`)**를 통해 최종 출력값(`y`)으로 변환됩니다.
  - **예시 (OR 연산)**: 가중치 `w1=1, w2=1`, 편향 `b=-0.5` (또는 활성 함수 내 임계값을 0.5로 설정).
    - 입력 (1, 1): `S = (1*1) + (1*1) - 0.5 = 1.5`. 활성 함수(`f`)를 계단 함수로 사용하면 `f(1.5) = 1`.
    - 입력 (0, 0): `S = (0*1) + (0*1) - 0.5 = -0.5`. 활성 함수 `f(-0.5) = 0`.
  - **예시 (AND 연산)**: 가중치 `w1=1, w2=1`, 편향 `b=-1.5`.
    - 입력 (1, 1): `S = (1*1) + (1*1) - 1.5 = 0.5`. 활성 함수 `f(0.5) = 1`.
    - 입력 (0, 0): `S = (0*1) + (0*1) - 1.5 = -1.5`. 활성 함수 `f(-1.5) = 0`.
- **다층 퍼셉트론 순전파 및 오류 역전파 (강의 5)**
  - **순전파 (Forward Pass)**: 입력층에서 은닉층, 그리고 출력층으로 순차적으로 계산이 진행됩니다.
    1. **입력층 → 은닉층 계산**: `U1 = X1*V11 + X2*V21 + 1*V01`, `U2 = X1*V12 + X2*V22 + 1*V02`. (V는 입력층과 은닉층 사이의 가중치).
    2. **은닉층 활성화**: `Z1 = f(U1)`, `Z2 = f(U2)`. (강의 예시에서는 선형 활성 함수를 사용하여 `Z1=U1, Z2=U2`로 계산 간편화).
    3. **은닉층 → 출력층 계산**: `A1 = Z1*W11 + Z2*W21 + 1*W01`, `A2 = Z1*W12 + Z2*W22 + 1*W02`. (W는 은닉층과 출력층 사이의 가중치).
    4. **출력층 활성화**: `Y1 = f(A1)`, `Y2 = f(A2)`. (강의 예시에서는 선형 활성 함수를 사용하여 `Y1=A1, Y2=A2`로 계산 간편화).
  - **오차 계산**: 예측값(`Y`)과 실제값(`D`)의 차이를 **오차 함수**를 통해 계산합니다. 보통 오차의 제곱합을 사용합니다: `Error = (D1-Y1)^2 + (D2-Y2)^2`.
    - 예시: 실제값 `(1,0)`에 예측값 `(2,2)`가 나왔다면, 오차는 `(1-2)^2 + (0-2)^2 = (-1)^2 + (-2)^2 = 1 + 4 = 5`.
  - **오류 역전파 (Backpropagation)**: 계산된 오차를 바탕으로 가중치를 업데이트합니다.
    - 오차는 출력층에서 입력층 방향으로 **역전파**됩니다.
    - 각 가중치에 대한 오차 함수의 **기울기(Gradient)**를 계산하고, **학습률(Learning Rate)**을 곱한 뒤 기존 가중치에서 빼서 새로운 가중치를 얻습니다 (경사 하강법).
    - 이때 **연쇄 법칙(Chain Rule)**이 사용되며, 이전 층으로 오차를 전파할 때는 **업데이트되기 이전의 가중치**를 사용합니다.
    - 강의에서는 복잡한 수식 유도는 컴퓨터가 수행하는 영역으로 간주하고, 개념 및 결과 값의 변화를 보여줍니다.
- **확신도 계산 (강의 3)**
  - **단일 추론 (A이면 B, A)**: `CF(B) = CF(A이면 B) * CF(A)`.
    - 예시: `CF(A이면 B) = 0.8`, `CF(A) = 0.6` 이면 `CF(B) = 0.8 * 0.6 = 0.48`.
  - **논리곱 (AND) 연산**: `CF(A 그리고 B 이면 C) * Minimum(CF(A), CF(B))`. (A, B 모두 양수일 때).
    - 예시: `CF(A and B -> C) = 0.8`, `CF(A) = 0.9`, `CF(B) = 0.7` 이면 `CF(C) = 0.8 * Min(0.9, 0.7) = 0.8 * 0.7 = 0.56`.
  - **논리합 (OR) 연산**: `CF(A 또는 B 이면 C) * Maximum(CF(A), CF(B))`. (A, B 모두 양수일 때).
    - 예시: `CF(A or B -> C) = 0.9`, `CF(A) = 0.6`, `CF(B) = 0.8` 이면 `CF(C) = 0.9 * Max(0.6, 0.8) = 0.9 * 0.8 = 0.72`.
  - **동일한 결론에 대한 두 확신도 결합**:
    - **둘 다 양수**: `CF_결합 = CF1 + CF2 - (CF1 * CF2)`.
      - 예시: `CF1 = 0.7`, `CF2 = 0.3` 이면 `0.7 + 0.3 - (0.7 * 0.3) = 1.0 - 0.21 = 0.79`.
    - **둘 다 음수**: `CF_결합 = CF1 + CF2 + (CF1 * CF2)`.
      - 예시: `CF1 = -0.4`, `CF2 = -0.2` 이면 `0.4 + (-0.2) + (-0.4 * -0.2) = -0.6 + 0.08 = -0.52`.
    - **서로 다른 부호**: `CF_결합 = (CF1 + CF2) / (1 - Min(CF1, CF2))`. (여기서 Min(CF1, CF2)는 실제 값 중 더 작은 값을 의미)
      - 예시: `CF1 = 0.9`, `CF2 = -0.6` 이면 `(0.9 + (-0.6)) / (1 - (-0.6)) = 0.3 / 1.6`.

### **2. AI의 미래**

인공지능의 궁극적인 목표는 **인간의 지능을 구현하는 것**이며, 최종 단계의 인공지능은 **인간과 유사한 존재**가 될 것으로 예상됩니다. 특히 **인공신경망(Artificial Neural Network, ANN)**은 이러한 목표를 위해 **인간 뇌의 복잡한 구조와 정보 처리 방식을 모방**하려는 시도에서 시작되었습니다.

- 딥러닝(Deep Learning)은 단순한 모형을 넘어 **깊은 층(Deep)의 신경망(Deep Neural Network, DNN)**을 학습시키는 알고리즘을 연구하는 분야를 지칭합니다. 이는 마치 **백지 상태의 어린아이 뇌가 성인처럼 학습하며 성장하는 과정**과 유사하다고 비유됩니다.
- 인공지능은 단일한 방법론에 국한되지 않고, **탐색, 논리적 추론, 심층 신경망 등 다양한 접근 방식을 결합**하여 활용될 것으로 보이며, 이 분야는 여전히 끊임없이 발전하고 있습니다.

### **3. 역전파, 최적해**

- \*역전파(Backpropagation)**는 다층 인공신경망의 **핵심 학습 알고리즘**입니다. 인공신경망의 **가중치(Weight)를 조정**하여 예측값과 실제값 사이의 **오차를 최소화하는 것을 목표**로 합니다. 이때 **오차를 최소화하는 지점**이 해당 문제에 대한 **최적해(Optimal Solution)\*\*를 의미합니다.
- **오차 계산**: 예측값과 실제값의 차이를 **오차 함수(Error Function)**를 통해 계산합니다. 일반적으로 실제값과 예측값의 차이의 제곱합을 사용합니다.
- **가중치 업데이트**: 오차를 최소화하기 위해 **경사 하강법(Gradient Descent)**이 사용됩니다. 오차 함수의 **기울기(미분값)**를 계산하고, 학습률(Learning Rate)과 곱한 값을 현재 가중치에서 빼거나 더하여 오차가 줄어드는 방향으로 가중치를 점진적으로 이동시킵니다.
- **오차 전파**: 계산된 오차 정보는 신경망의 **출력층에서 입력층 방향으로 역방향으로 전파**됩니다. 이때, 오차를 발생시킨 **업데이트되기 이전의 가중치**를 사용하여 오차 정보를 전달합니다.
- **복잡한 계산**: 다층 신경망에서는 **연쇄 법칙(Chain Rule)**을 사용하여 각 층의 가중치에 대한 오차 함수의 기울기를 계산합니다. 이 과정은 매우 복잡하여 보통 컴퓨터가 수행합니다.
- 이러한 오차 계산 및 가중치 업데이트 과정은 오차가 충분히 줄어들 때까지 **반복적으로 수행**되어 MLP 모형이 완성됩니다.

### **4. 퍼셉트론 갯수**

- \*퍼셉트론(Perceptron)**은 인공신경망의 **가장 기본적인 단위\*\*로, 하나의 생물학적 뉴런의 작동 방식을 모방합니다.
- **단일 퍼셉트론**: **하나의 퍼셉트론**은 입력 패턴을 **직선으로 분리할 수 있는 문제(선형 분리 가능 문제)**만을 해결할 수 있습니다. 예를 들어, OR 연산이나 AND 연산은 단일 퍼셉트론으로 해결 가능합니다.
- **다층 퍼셉트론(Multi-Layer Perceptron, MLP)**: 단일 퍼셉트론의 한계(예: XOR 문제)를 극복하기 위해 **여러 개의 퍼셉트론을 층(Layer)으로 쌓은 모형**입니다.
  - MLP는 **입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)**으로 구성됩니다.
  - 여러 퍼셉트론이 서로 **협동하여 복잡한 문제를 해결**합니다. 이는 인간 뇌의 뉴런들이 서로 연결되는 방식과 유사합니다.
- **심층 신경망(Deep Neural Network, DNN)**: MLP에서 **은닉층이 여러 개 더 쌓여 복잡해진 모형**을 의미하며, "Deep"이라는 용어는 단순히 층이 많다는 것을 나타냅니다.

### **5. 지능이란 무엇인가**

강의에서는 지능을 주로 **인간의 뇌**와 그 기능을 통해 설명합니다.

- **인간 지능의 원천**: 인간의 지능은 **뇌**에서 나오며, 뇌는 몸의 움직임과 행동 관장, 신체 항상성 유지, 인지, 감정, 기억, 학습 기능을 담당합니다. 인지신경과학에서는 뇌의 궁극적인 목적을 **생존에 무엇이 도움이 되는가에 대한 예측**이라고 설명합니다.
- **뇌의 복잡성**: 뇌는 약 **100억 개의 뉴런(신경 세포)**과 이들을 연결하는 **6조 개의 시냅스**로 구성된 복잡한 네트워크입니다. 이 뉴런들은 동시에 작동할 수 있어 현재의 컴퓨터보다 빠른 정보 처리 능력을 가집니다.
- **인공지능의 목표**: 인공지능은 이러한 **인간의 지능을 모방**하고 구현하는 것을 목표로 합니다. 초기 AI 체계는 **철학과 수학의 논리적 기법과 추론 방법**의 영향을 많이 받았습니다.
- **인간과 컴퓨터의 문제 해결 방식**:
  - **인간**: 비형식적(informal)으로 문제를 해결합니다. 사람마다 지식, 경험, 접근 방식이 다르며, 의지에 따라 자유로운 사고를 통해 문제를 풀어 나갑니다.
  - **컴퓨터**: 형식적(formal)이고 계산적인 방식으로 문제를 해결합니다. 인간이 0과 1로 표현된 신호 체계 안에서 자료와 방법론을 제공해야만 컴퓨터는 반복 계산을 수행하고 결과를 출력합니다. 컴퓨터는 인간처럼 스스로 추론 규칙을 만들지 못하므로, 인간이 규칙을 명확히 알려주어야 합니다.

### **6. XOR 다층 퍼셉트론 내의 퍼셉트론 갯수**

**XOR(Exclusive OR) 문제**는 단일 퍼셉트론으로 해결할 수 없는 **선형 분리 불가능(Linearly Inseparable) 문제**의 대표적인 예시입니다. 하나의 직선으로는 데이터를 올바르게 분류할 수 없습니다.

- XOR 문제는 **두 개의 직선**을 사용하면 분리가 가능하며, 이를 다층 퍼셉트론으로 구현할 수 있습니다.
- 강의에서 제시된 XOR 문제 해결을 위한 다층 퍼셉트론은 **총 3개의 퍼셉트론**을 사용합니다.
  - **두 개의 퍼셉트론**은 **은닉층(Hidden Layer)**에 위치하여, 입력 공간을 선형 분리 가능한 새로운 공간으로 변환합니다.
  - **한 개의 퍼셉트론**은 **출력층(Output Layer)**에 위치하여, 은닉층에서 변환된 데이터를 최종적으로 분류합니다.

### **7. 확신도 계산**

- \*확신도(Certainty Factor, CF)**는 불확실한 지식을 수치적으로 표현하는 방법으로, **-1(절대 아니다)**부터 **1(확실하다)**까지의 값을 가집니다. **0은 모른다\*\*는 의미입니다. 확신도는 논리적 연산이나 여러 증거가 결합될 때 다음과 같이 계산됩니다.
- **단일 추론**: A이면 B일 때, `CF(B) = CF(A이면B) * CF(A)`. (예: 0.8 \* 0.6 = 0.48)
- **AND 연산 (`AND`로 결합된 전제에 대한 결론)**: `CF_C = CF(A 그리고 B이면 C) * Min(CF_A, CF_B)`. 논리곱의 특징(하나라도 거짓이면 전체 거짓)을 반영하여, 두 확신도 중 **더 낮은 값**이 전체에 기여합니다.
- **OR 연산 (`OR`로 결합된 전제에 대한 결론)**: `CF_C = CF(A 또는 B이면 C) * Max(CF_A, CF_B)`. 논리합의 특징(하나라도 참이면 전체 참)을 반영하여, 두 확신도 중 **더 높은 값**이 전체에 기여합니다.
- **동일한 결론에 대한 여러 증거 결합**:
  - **모두 양수일 때**: `CF_결합 = CF1 + CF2 - (CF1 * CF2)`. (예: 0.7 + 0.3 - 0.21 = 0.79)
  - **모두 음수일 때**: `CF_결합 = CF1 + CF2 + (CF1 * CF2)`. (예: -0.4 + (-0.2) + (-0.4 \* -0.2) = -0.6 + 0.08 = -0.52)
  - **서로 다른 부호일 때**: `CF_결합 = (CF1 + CF2) / (1 - Min(CF1, CF2))`. (예: (0.9 + (-0.6)) / (1 - (-0.6)) = 0.3 / 1.6). 이 공식은 더 강한 방향으로 결과가 기울되, 반대되는 증거에 의해 그 정도가 완화되도록 합니다.

### **8. 다층 퍼셉트론 전체 계산**

- \*다층 퍼셉트론(MLP)\*\*은 입력층, 은닉층, 출력층으로 구성된 인공신경망으로, 정보는 층을 따라 순차적으로 전달되며 각 단계에서 계산이 이루어집니다.
- **입력층 (Input Layer)**: 외부로부터 데이터를 받아들이는 층입니다. 각 입력값(`X`)은 다음 층의 각 뉴런으로 전달됩니다.
- **은닉층 (Hidden Layer)**: 입력층과 출력층 사이에 위치하는 하나 이상의 층입니다.
  - 입력층으로부터 전달된 값들은 해당 연결의 **가중치(Weight)**와 곱해지고, **편향(Bias)**이 더해져 각 뉴런의 **합성값**이 됩니다.
  - 각 합성값은 **활성화 함수(Activation Function)**를 거쳐 다음 층으로의 출력이 됩니다 (`Z` 값). 이 과정에서 정보의 비선형적인 변환이 일어납니다.
- **출력층 (Output Layer)**: 은닉층의 출력을 받아 최종 결과를 도출하는 층입니다.
  - 은닉층에서 전달된 값들은 다시 해당 연결의 **가중치**와 곱해지고 **편향**이 더해져 각 출력 뉴런의 **합성값**이 됩니다.
  - 이 합성값 또한 **활성화 함수**를 거쳐 네트워크의 최종 출력값(예측값 `Y`)이 됩니다.
- MLP의 계산은 이처럼 입력값이 각 층의 뉴런을 통해 가중합되고 활성화 함수를 거쳐 다음 층으로 전달되는 **순전파(Feedforward)** 방식으로 이루어집니다. 초기 가중치는 임의로 설정되며, 학습을 통해 실제값과 예측값의 오차를 줄이는 방향으로 가중치가 계속 업데이트됩니다.
